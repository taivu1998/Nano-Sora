# Nano-Sora Best Quality Config for A100 GPU (40-80GB VRAM)
# Fine patches + large model = best possible quality

experiment:
  name: "nano_sora_a100_best"
  seed: 42
  output_dir: "./logs"
  save_every: 25

data:
  batch_size: 64           # With Flash Attention, can use larger batch
  num_workers: 4
  num_frames: 16
  image_size: 64
  train_split: 0.9

model:
  patch_size: [2, 4, 4]    # Fine patches = 2048 tokens = better detail
  hidden_size: 512         # Large hidden size
  depth: 12                # Deep model
  num_heads: 8
  mlp_ratio: 4.0
  dropout: 0.1

training:
  epochs: 200
  lr: 2.0e-4               # Standard LR for larger batch
  weight_decay: 0.05
  warmup_epochs: 10
  use_amp: true
  grad_clip: 1.0
  validate_every: 10
  flow_steps: 50
  use_ema: true
  ema_decay: 0.9999
