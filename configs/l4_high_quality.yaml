# Nano-Sora Config for L4 GPU (24GB VRAM)
# Larger model but standard patches to fit in memory

experiment:
  name: "nano_sora_l4_hq"
  seed: 42
  output_dir: "./logs"
  save_every: 25

data:
  batch_size: 64           # Reduced from 128 for safety
  num_workers: 4
  num_frames: 16
  image_size: 64
  train_split: 0.9

model:
  patch_size: [2, 8, 8]    # Standard patches (512 tokens) - FITS IN MEMORY
  hidden_size: 512         # Larger hidden size
  depth: 12                # Deeper model
  num_heads: 8
  mlp_ratio: 4.0
  dropout: 0.1

training:
  epochs: 200
  lr: 2.0e-4
  weight_decay: 0.05
  warmup_epochs: 10
  use_amp: true
  grad_clip: 1.0
  validate_every: 10
  flow_steps: 50
  use_ema: true
  ema_decay: 0.9999

# For A100 (40GB+), you can use fine patches for better detail:
# model:
#   patch_size: [2, 4, 4]  # 2048 tokens - requires A100
#   hidden_size: 512
#   depth: 12
